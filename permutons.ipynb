{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutons for variance reduction on PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import time\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import array, zeros, diag, diagflat, dot\n",
    "from math import sqrt\n",
    "import pprint\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "import os\n",
    "os.chdir('/homes/ir337/Documents/learnable_qmc')\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from rf_construction import *\n",
    "from extra_funcs import *\n",
    "from scipy import stats\n",
    "id_norm = stats.norm()\n",
    "from length_samplers import *\n",
    "import itertools\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "As = np.load('adj_matrices.npy',allow_pickle=True) #load the adjacency matrices\n",
    "p_halt = 0.2\n",
    "nb_trials = 4  #num of walkers out of each node\n",
    "\n",
    "nb_bins = 10        #hyperparameter for order of permutation to learn\n",
    "nb_per_bin = 10     #number of samples to take in every permutation bin\n",
    "\n",
    "variance_repeats = 1000     #number of repeats when comparing approximation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groundtruth_stationary(A, p_halt):\n",
    "    \"find the exact PageRank vector -- stationary dist of the Markov chain\"\n",
    "    nb_vertices = len(A)\n",
    "    degrees = np.sum(A,axis=1)\n",
    "    P = (A.T / degrees).T\n",
    "    pi = p_halt/nb_vertices * np.sum(np.linalg.inv(np.eye(nb_vertices) - (1-p_halt) * P), axis=0) \n",
    "    return pi\n",
    "\n",
    "def get_walk(adj_lists,base_vertex, length):\n",
    "    \"get walk endpoints from lengths we've sampled\"\n",
    "    \n",
    "    current_vertex = base_vertex\n",
    "    for _ in range(length):\n",
    "      \n",
    "        rnd_index = int(rnd.uniform(0,1) * len(adj_lists[current_vertex]))\n",
    "        current_vertex = adj_lists[current_vertex][rnd_index]\n",
    "\n",
    "    return current_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = As[1]\n",
    "nb_vertices = np.shape(A)[0]\n",
    "adj_lists, weight_lists, visits_list = adj_matrix_to_lists(A, qmc=True)\n",
    "pi = get_groundtruth_stationary(A, p_halt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_projections(nb_bins, nb_per_bin,p_halt):\n",
    "    \"generate projections for every bin encoding where walker ends up given starting node and respective length. Gives nb_bins * nb_nodes * nb_nodes matrix matrix\"\n",
    "\n",
    "    unif_bins = np.linspace(0,1 - 1/(nb_bins * nb_per_bin),nb_bins * nb_per_bin)\n",
    "    lengths = np.asarray(np.floor(np.log(1 - unif_bins) / np.log(1 - p_halt)), dtype = int)\n",
    "\n",
    "    features = []\n",
    "    for bin in range(nb_bins):\n",
    "        this_feature = np.zeros((nb_vertices, nb_vertices))\n",
    "        for _ in range(nb_trials):\n",
    "            for base_vertex in range(nb_vertices):\n",
    "                for length in lengths[bin*nb_per_bin: (bin+1)*nb_per_bin]:\n",
    "                    end_vertex = get_walk(adj_lists, base_vertex, length)\n",
    "                    this_feature[base_vertex,end_vertex] += 1\n",
    "        features.append(this_feature)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_cost_matrix(projections):\n",
    "    \"return a cost matrix given projections\"\n",
    "\n",
    "    nb_bins = len(projections)\n",
    "    cost_matrix = np.zeros((nb_bins,nb_bins))\n",
    "    for i in range(nb_bins):\n",
    "        for j in range(nb_bins):\n",
    "            cost_matrix[i,j] += np.sum(projections[i] * projections[j])\n",
    "    \n",
    "    return cost_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = sample_projections(nb_bins,nb_per_bin,p_halt)        #compute projections\n",
    "cost_matrix = get_cost_matrix(projections)      #assemble in cost matrix\n",
    "row_indices, optimal_permutation = scipy.optimize.linear_sum_assignment(cost_matrix)        #solve linear optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 0, 9, 8, 1, 7, 6, 4, 3])"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_permutation     #the best choice of permuton for PageRank among class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling lengths to construct PageRank estimates\n",
    "\n",
    "def sample_iid_lengths(p, nb):\n",
    "    \"pair of independent lengths\"\n",
    "    lengths = []\n",
    "    for _ in range(nb):\n",
    "        out = []\n",
    "        for _ in range(2):\n",
    "            counter = 0\n",
    "            while np.random.random()>p:\n",
    "                counter += 1\n",
    "            out.append(counter)\n",
    "        lengths.append(out)\n",
    "    return lengths\n",
    "\n",
    "def sample_antithetic_lengths(p, nb):\n",
    "    \"pair of antithetic lengths\"\n",
    "    lengths = []\n",
    "    for _ in range(nb):\n",
    "\n",
    "        terminated = np.zeros(2)\n",
    "        counters = np.zeros(2)\n",
    "        while np.sum(terminated) < 2:\n",
    "            rv = np.random.random()\n",
    "            rvs = [rv, np.mod(rv+0.5,1)]\n",
    "            if terminated[0] == 0:\n",
    "                terminated[0] = rvs[0] < p\n",
    "                counters[0] += rvs[0] > p\n",
    "            if terminated[1] == 0:\n",
    "                terminated[1] = rvs[1] < p\n",
    "                counters[1] += rvs[1] > p\n",
    "        counters[0] = (counters[0])\n",
    "        counters[1] = (counters[1])\n",
    "        lengths.append([int(counters[0]),int(counters[1])])\n",
    "    return lengths\n",
    "\n",
    "def sample_permuton(permutation):\n",
    "    \"arbitrary n permuton, marginally uniformly distributed\"\n",
    "    n = len(permutation)   #length of permutation = number of elements in the grid\n",
    "    u1 = np.random.random()\n",
    "    box = int(np.floor(u1 * n))\n",
    "    target = permutation[box]\n",
    "    u2 = (target + np.random.random()) / n\n",
    "    return (u1,u2)\n",
    "\n",
    "def sample_permuton_lengths(p,nb,permutation):\n",
    "    \"get the corresponding permuton lengths\"\n",
    "    lengths = []\n",
    "    for _ in range(nb):\n",
    "        out = []\n",
    "        rvs = sample_permuton(permutation)\n",
    "        out.append( int(np.floor(np.log(1-rvs[0]) / np.log(1-p) )))\n",
    "        out.append( int(np.floor(np.log(1-rvs[1]) / np.log(1-p) )))\n",
    "        lengths.append(out)\n",
    "    return lengths\n",
    "\n",
    "def get_pred(lengths):\n",
    "    \"predict PageRank vector for graph given list of walk lengths\"\n",
    "    endpoints = np.zeros(nb_vertices)\n",
    "    for base_vertex in range(nb_vertices):\n",
    "        for length in lengths:\n",
    "            endpoint = get_walk(adj_lists, base_vertex,length)\n",
    "            endpoints[endpoint] += 1\n",
    "    return endpoints / np.sum(endpoints)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 318/1000 [00:00<00:00, 759.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 749.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08316201478325891\n",
      "0.0005924609439922973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# i.i.d.\n",
    "error_log = []\n",
    "for _ in tqdm(range(variance_repeats)):\n",
    "    lengths = np.ndarray.flatten(np.asarray(sample_iid_lengths(p_halt,nb_trials)))\n",
    "    pred = get_pred(lengths)\n",
    "    error_log.append( np.sum((pred - pi)**2) / np.sum(pi**2))\n",
    "print(np.mean(error_log))\n",
    "print(np.std(error_log) / np.sqrt(variance_repeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 713.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08295445058558874\n",
      "0.0005859935634669137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# antithetic\n",
    "error_log = []\n",
    "for _ in tqdm(range(variance_repeats)):\n",
    "    lengths = np.ndarray.flatten(np.asarray(sample_antithetic_lengths(p_halt,nb_trials)))\n",
    "    pred = get_pred(lengths)\n",
    "    error_log.append( np.sum((pred - pi)**2) / np.sum(pi**2))\n",
    "print(np.mean(error_log))\n",
    "print(np.std(error_log) / np.sqrt(variance_repeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 777.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08212933247676764\n",
      "0.0005971939481975643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Permuton\n",
    "error_log = []\n",
    "for _ in tqdm(range(variance_repeats)):\n",
    "    lengths = np.ndarray.flatten(np.asarray(sample_permuton_lengths(p_halt,nb_trials,optimal_permutation)))\n",
    "    pred = get_pred(lengths)\n",
    "    error_log.append( np.sum((pred - pi)**2) / np.sum(pi**2))\n",
    "print(np.mean(error_log))\n",
    "print(np.std(error_log) / np.sqrt(variance_repeats))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
